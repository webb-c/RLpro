{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PPO와 관련된 기본 설계는 https://github.com/seungeunrho/minimalRL의 코드를 이용했습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "(96, 96, 3) <class 'numpy.ndarray'>\n",
      "mini batch\n",
      "mini batch\n",
      "mini batch\n",
      "torch.Size([3, 5, 96, 96, 3]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [3, 5, 96, 96, 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 187\u001B[0m\n\u001B[1;32m    184\u001B[0m     env\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 187\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[43], line 178\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    177\u001B[0m     \u001B[38;5;66;03m# print(\"train_net\")\u001B[39;00m\n\u001B[0;32m--> 178\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_epi\u001B[38;5;241m%\u001B[39mprint_interval\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m n_epi\u001B[38;5;241m!=\u001B[39m\u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    181\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# of episode :\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, avg score : \u001B[39m\u001B[38;5;132;01m{:.1f}\u001B[39;00m\u001B[38;5;124m, opt step: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_epi, score\u001B[38;5;241m/\u001B[39mprint_interval, model\u001B[38;5;241m.\u001B[39moptimization_step))\n",
      "Cell \u001B[0;32mIn[43], line 123\u001B[0m, in \u001B[0;36mPPO_Car.train_net\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata) \u001B[38;5;241m==\u001B[39m minibatch_size \u001B[38;5;241m*\u001B[39m buffer_size:\n\u001B[1;32m    122\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_batch()\n\u001B[0;32m--> 123\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcalc_advantage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(K_epoch):\n\u001B[1;32m    126\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m mini_batch \u001B[38;5;129;01min\u001B[39;00m data:\n",
      "Cell \u001B[0;32mIn[43], line 104\u001B[0m, in \u001B[0;36mPPO_Car.calc_advantage\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    103\u001B[0m     \u001B[38;5;28mprint\u001B[39m(s_prime\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;28mtype\u001B[39m(s_prime))\n\u001B[0;32m--> 104\u001B[0m     td_target \u001B[38;5;241m=\u001B[39m r \u001B[38;5;241m+\u001B[39m gamma \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mv\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_prime\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m done_mask\n\u001B[1;32m    105\u001B[0m     delta \u001B[38;5;241m=\u001B[39m td_target \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv(s)\n\u001B[1;32m    106\u001B[0m delta \u001B[38;5;241m=\u001B[39m delta\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "Cell \u001B[0;32mIn[43], line 52\u001B[0m, in \u001B[0;36mPPO_Car.v\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mv\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 52\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(output)\n\u001B[1;32m     54\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv3(output)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RL_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RL_env/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 204\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RL_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RL_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/RL_env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [3, 5, 96, 96, 3]"
     ]
    }
   ],
   "source": [
    "# PPO for car_racing\n",
    "## Action : 3 type; continuous\n",
    "## Observation : (96, 96, 3) image -> CNN?\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate  = 0.0003\n",
    "gamma           = 0.9\n",
    "lmbda           = 0.9\n",
    "eps_clip        = 0.2\n",
    "K_epoch         = 10\n",
    "rollout_len    = 5\n",
    "buffer_size    = 3\n",
    "minibatch_size = 3\n",
    "\n",
    "class PPO_Car(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO_Car, self).__init__()\n",
    "        self.data = []\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(12*12*128,3)\n",
    "        self.fc_std  = nn.Linear(12*12*128,3)\n",
    "        self.fc_v = nn.Linear(12*12*128,3)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def forward(self, x, softmax_dim = 0):\n",
    "        #x = np.transpose(x, (0,3,2,1))\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        #output = output.view(output.size(0), -1)\n",
    "        output = output.flatten()\n",
    "        mu = 2.0 * torch.tanh(self.fc_mu(output))\n",
    "        std = F.softplus(self.fc_std(output))\n",
    "        return mu, std\n",
    "\n",
    "    def v(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2(output)\n",
    "        output = self.conv3(output)\n",
    "        # output = output.view(output.size(0), -1)\n",
    "        output = output.flatten()\n",
    "        v = self.fc_v(output)\n",
    "        return v\n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "\n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "\n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "\n",
    "            print(\"mini batch\")\n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), a_batch, \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                print(s_prime.shape, type(s_prime))\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "\n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "\n",
    "                    mu, std = self.forward(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1\n",
    "\n",
    "def main():\n",
    "    # env = gym.make(\"CarRacing-v2\", render_mode=\"human\") # car racing visualizing\n",
    "    env = gym.make(\"CarRacing-v2\")\n",
    "    model = PPO_Car()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    rollout = []\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, info = env.reset(seed=410, options={})\n",
    "        done = False\n",
    "        while not done:\n",
    "            for t in range(rollout_len):\n",
    "                state = torch.from_numpy(s).float()\n",
    "                state = np.transpose(state, (2,1,0))\n",
    "                mu, std = model.forward(state)  # 다변량 정규분포 평균, 표준편차\n",
    "                dist = MultivariateNormal(mu, torch.diag(std))     # 정규분포 함수 생성\n",
    "                action = dist.sample()                      # 샘플링; 2차원 텐서\n",
    "                log_prob = dist.log_prob(action)\n",
    "                observation, reward, terminated, truncated, info = env.step(action.numpy())\n",
    "                observation = np.transpose(observation, (2,1,0))\n",
    "                if terminated or truncated : done = True\n",
    "                rollout.append((state, action, reward/10.0, observation, log_prob.item(), done))\n",
    "                if len(rollout) == rollout_len :\n",
    "                    model.put_data(rollout)\n",
    "                    rollout = []\n",
    "                s = observation\n",
    "                score += reward\n",
    "                if done :\n",
    "                    print(\"break!\")\n",
    "                    break\n",
    "            # print(\"train_net\")\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, opt step: {}\".format(n_epi, score/print_interval, model.optimization_step))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
